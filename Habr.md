# Варим кашу из нечеткой логики и вариационных автоэнкодеры 

Когда то давным давно, когда вариационные автоэнкодеры (VAE) только только появились, для меня это было как гром среди синего неба - наконец-то сетки смогут извлекать знание из данных, представляя их в виде параметризированных моделей, как это делают в формулах физики\математики! 
Воображение рисовало картины нейросетевых моделей, дают возможность изменением крутилок отращивать волосы нужной длины на фотографиях людей или извекать из тензометрических данных параметры, зависящие от температуры среды, но... 

[картинка со стульчиками из статьи]

Реальность оказалась не такой, даже обучение VAE на синтетических данных из синусоид разных частот и амлитуд не дает отдельных параметров, которые бы отвечали за только за частоту или только за амплитуду. 
Безусловно, если перед вами стоит задача сгенерировать определенную синусоиду с нужной вам частотой,то вы наверняка подоберете комбинацию параметров скрытого слоя автоэкодера, которая выдаст вам желаемое, но красота момента пропадает.
Спустя какое то время мне пришла в голову мысль, что нужно примешивать информацию о предметной области в процессе обучения VAE, чтобы фичи автоэнкодера были не только хорошо сегрегированны, но еще и несли человекоосмысленную нагрузку. 
Развитие этой идеи привело к реализации специального слоя для нейросетевых моделей, который в связке с VAE позволяет выполнять обучение на частично размеченных датасетах и на выходе дает возможность интерпретировать параметры скрытого слоя в домене предметного знания. 
Весь код проекта находится в репозиториях для [Keras](https://github.com/kenoma/KerasFuzzy) и для [pytorch](https://github.com/kenoma/pytorch-fuzzy).

## Нечеткий слой

Прежде чем переходить к нейросетевым моделям, попробую на пальцах объяснить, что такое нечеткий слой. 
Сам термин "нечеткий" уже явно намекает на то, что в деле замешана нечеткая логика, по факту от которой были отщипнуты термы, а еще конкретнее - характеристические функции (или функции принадлежности). 
Эти функции описывают степень принадлежности некоего входного занчения $x$ к определенному нечеткому множеству, выражая эту степень от 0 (не принадлежит) до 1 (принадлежит полностью).
Разновидностей функций принадлежности существует огромное количество на любой вкус и цвет, но для простоты можно начать с классической гауссианы, для одномерного случая имеющая вид $ /\mu\left(x\right)=e^{-{\frac {(x-a)^{2}}{2b^{2}}}}}$

[картинка гауссианы]

Параметры $a$ и $b$ в этой формуле отвечают за смещение центра гауссианы относительно нуля и ее ширину, варьируя их можно более аккуратно подогнать гауссиану под конкретное нечеткое множество.
Приведенный одномерный случай прекрасен для понимания - интуитивно понятно, что чем ближе $x$ к $a$, тем выше его степень принадлежности к указанному множеству, а сам параметр $a$ описывает наиболее характного представителя множества. Однако в нейронных сетях крайне редко можно встретить 





