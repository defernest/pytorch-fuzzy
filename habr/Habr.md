# Варим кашу из нечеткой логики и вариационных автоэнкодеры 

Когда то давным давно, когда вариационные автоэнкодеры (VAE) только только появились, для меня это было как гром среди синего неба - наконец-то сетки смогут извлекать знание из данных, представляя их в виде параметризированных моделей, как это делают в формулах физики\математики!
Воображение рисовало возможностей изменением крутилок отращивать волосы нужной длины на фотографиях людей или извекать из тензометрических данных параметры, зависящие от температуры среды, но...

![Картинка из [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)](fig1.png)
_Картинка из [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)_

Реальность же оказалась сложнее, богаче и разнообразнее на сложности и разочарования, одним из которых явилось то, что VAE, даже в продвинутой вариации $\beta$-VAE, в скрытом слое выдает набор параметров, интерпретация которых требует перебора этих параметров в некоторой заранее неизвестной допустимой области изменения этих параметров и экспертного мнения для отслеживания. 
Покрутил параметр и увидел что у тебя стульчик на картинке завращался в ответ - отлично, параметр вращения.
Изменил другой параметр - размер стульчика начал меняться и так далее.
Хорошо, если задача довольно простая и не требует особых экспертных познаний в предметной области, однако такое бывает далеко не всегда, скорее даже чаще приходится решать задачи в которых экспертные знания скудны или на вес золота.
Ну раз без экспертного мнения не обойтись при обучении VAE моделек, то почему бы его и не предоставлять?
Так родилась идея CVAE, когда вместе с реконструкцией автоэнкодера обучается еще и классификатор.
Однако, CVAE это нечто очень близкое к обычным классификаторам, которые на раз-два уделают любой автоэнкодер, потому что последнему кроме классификации нужно еще и реконструкцию обеспечить на должном уровне.
Да и вопросы интерпретации фич особо никуда не делись.
С чем здорово помогает CVAE так это с частично размеченными датасетами, когда мы можем существенно ускорить процедуру разметки оставшегося датасета подсказками от CVAE.

Пока весь мир затаив дыхание следит за большими языковыми моделями и одни грезят о том, как подсадят всех на свои сервисы LLM, а другие прикидывают как можно сократить с их помощью если не зажравшихся айтишников, то хотя бы бухгалтеров, обычным машинистам по щиколотку в коричневой жиже дипленинга приходится решать вполне себе промышленно-бытовые задачи, которые требуются вот уже сейчас.
Наша команда - не исключение и решая очередную плохо обсусловленную практическую задачу из разряда "найдите в датасете картинок с исправными пружинами пружины с дефектами" сформировалась идея применения элементов нечеткой логики для построения CVAE.

Развитие этой идеи привело к реализации специального слоя для нейросетевых моделей, который в связке с VAE позволяет построить модели, пригодные к обучению на частично размеченных датасетах и раскрывающие информацию о кластерной структуре латентного вектора. Последнее может быть полезным как само по себе для задач классификации\анализа параметров, так и для целенаправленной генерации реконструированных объектов.

Весь код проекта и примеров к статье находится в репозиториях для [Keras](https://github.com/kenoma/KerasFuzzy) и для [pytorch](https://github.com/kenoma/pytorch-fuzzy).

## Нечеткий слой

Прежде чем переходить к нейросетевым моделям, попробую на пальцах объяснить, что такое нечеткий слой.
Сам термин "нечеткий" уже явно намекает на то, что в деле замешана нечеткая логика, по факту от которой были отщипнуты термы, а еще конкретнее - характеристические функции (или функции принадлежности).
Эти функции описывают степень принадлежности некоего входного занчения $x$ к определенному нечеткому множеству, выражая эту степень от 0 (не принадлежит) до 1 (принадлежит полностью).
Разновидностей функций принадлежности существует огромное количество на любой вкус и цвет, но для простоты можно начать с классической [гауссианы](https://ru.wikipedia.org/wiki/%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%BE%D0%B2%D0%B0_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F), для одномерного случая имеющая вид $\mu(x)=e^{-(\frac{x-a}{b\sqrt2})^2}$.

Параметры $a$ и $b$ в этой формуле отвечают за смещение центра гауссианы относительно нуля и ее ширину, варьируя их можно более аккуратно подогнать гауссиану под конкретное нечеткое множество.
Приведенный одномерный случай прекрасен для понимания - интуитивно понятно, что чем ближе $x$ к $a$, тем выше его степень принадлежности к указанному множеству, а сам параметр $a$ описывает наиболее характного среднестатистического представителя множества.
Практически же ценными являются многомерные гауссианы, которые мы представляем в виде
$\mu(x,A)=e^{-||[A.\~x]_{1\cdots m}||^2}$ где $m$ это размерность целевого пространства, а  $A$ это [матрица переноса](https://en.wikipedia.org/wiki/Transformation_matrix):

```math
A_{(m+1) \times (m+1)} =
  \left[ {\begin{array}{cccc}
    s_{1} & a_{12} & \cdots & a_{1m} & c_{1}\\
    a_{21} & s_{2} & \cdots & a_{2m} & c_{2}\\
    \vdots & \vdots & \ddots & \vdots & c_{3}\\
    a_{m1} & a_{m2} & \cdots & s_{m} & c_{m}\\
    0 & 0 & \cdots & 0 & 1\\
  \end{array} } \right]
```

где $c_{1\cdots m}$ - это центроид гауссианы, $s_{1\cdots m}$ ее масштабирующий фактор, а $a_{1\cdots m, 1\cdots m}$ параметры вращения гауссианы вокруг центроида.
Не касаясь некоторых аспектов ограничений на матрицу $A$ (например, подматрица из $s_{1\cdots m}$ и $a_{1\cdots m, 1\cdots m}$ должна быть положительно определенной), которые мы оставим для научных работ, можно смело заявлять, что в таком виде мы можем это все дело засунуть в популярный ML-фреймворк и с помощью волшебства обратного распространения градиента затюнить вусмерть каким пожелаем образом.
![Двумерная гауссиана](fig2.png)
_Пример двумерной гауссианы_

Другими словами, наша гауссиана это некая функция, которая входным векторам $x$ выдает значения тем ближе к 1, чем ближе они находятся к центроиду гауссианы и тем ближе к 0, чем дальше они от $c_{1\cdots m}$.

Совокупность таких многомерных гауссиан мы объединили в нечеткий слой, который в коде обозначили как `FuzzyLayer`.

Самая главная фишка `FuzzyLayer` заключается в том, что каждый элемент слоя стремиться сгруппировать под собой все входные значения, отвечающими за сработку того или выходного признака, действуя тем самым как кластеризатор.

## Вживляем нечеткость в VAE 

Как вы могли уже догадаться, наш `FuzzyLayer` приставленный к латентному слою VAE может группировать его значения в определенных областях, формируя кластеры связанных с нечеткой термой.
И что самое интересное, такая группировка идейно не сильно вмешивается в работу самого VAE, ведь и в самом деле, какая разница, где ему размещать свои кучки распределений.

[было - стало]

Чтобы продемонстрировать работу `FuzzyLayer` давайте попрепарируем многострадальный MNIST и попробуем на нем получить классификатор для цифр и не отходя от кассы классификатор, который сообщает нам, есть в начертании цифры замкнутый контур.
Другими словами, `0,6,8,9` налево, `1,2,3,4,5,7` направо.
Латентный вектор будет размерности 2 для того, чтобы получить красивые наглядные картиночки, а за точностью и комариными хоботками размерность придется повышать.

Полный код блокнота с представленными экспериментами вы найдете на [гитхабе](), тут 