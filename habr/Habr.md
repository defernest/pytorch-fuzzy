# Варим кашу из нечеткой логики и вариационных автоэнкодеры

Пока весь мир затаив дыхание следит за большими языковыми моделями и одни грезят о том, как подсадят всех на свои сервисы LLM, а другие прикидывают как можно сократить если не зажравшихся айтишников, то хотя бы бухгалтеров, обычным машинистам по щиколотку в коричневой жиже дипленинга приходится решать вполне себе промышленно-бытовые задачи, которые нужны вот уже сейчас.
Наша команда не исключение и решая очередную _интересную_ задачу из разряда "сделайте из датасета картинок с исправными пружинами детектор неисправных амортизационных комплексов" разработала инструмент, упрощающий разметку датасетов, генерацию синтетических данных и детекцию аномалий.

В этой статье я хочу представить идею использования элементов нечеткой логики в обучении вариационных энкодеров (VAE) и поделиться кодом, с которым может ознакомиться любой желающий. Основной репозитории на [pytorch](https://github.com/kenoma/pytorch-fuzzy), но есть реализация и на [Keras](https://github.com/kenoma/KerasFuzzy).

Когда то давным давно, когда вариационные автоэнкодеры только только появились, для меня это было как гром среди синего неба - наконец-то сетки смогут извлекать знание из данных, представляя их в виде параметризированных моделей, прямо как это делают в формулах физики\математики!
Воображение рисовало мне возможности легким движением руки отращивать волосы нужной длины на фотографиях людей, извлекать из тензометрических данных параметры, зависящие от температуры среды, сжимать покерные руки в эффективные бакеты малой размерности и надирать всем зад, но...

![Картинка из [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)](fig1.png)

_Картинка из [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)_

Реальность же оказалась сложнее, богаче и разнообразнее на разочарования.
Чистая VAE даже в продвинутой реализации [$\beta$-VAE](https://www.semanticscholar.org/paper/beta-VAE%3A-Learning-Basic-Visual-Concepts-with-a-Higgins-Matthey/a90226c41b79f8b06007609f39f82757073641e2?p2df) в скрытом слое выдает набор параметров, интерпретация которых требует перебора этих параметров в некоторой заранее неизвестной области изменения этих параметров и _экспертного_ мнения для отслеживания правильности того, правильно ли шевелится ли там на выходе то, что вы шевелите.
Процедура по задумке должна выглядеть так: покрутил параметр и увидел, что у стульчик на картинке завращался в ответ? 
Отлично, параметр вращения.
Изменил другой параметр - размер стульчика начал меняться?
Шикарно.
И так далее.

Хорошо, если задача довольно простая и не требует особых познаний в решаемой предметной области, однако такое бывает далеко не всегда.
Скорее чаще приходится решать задачи в которых экспертные знания скудны или на вес золота.
Ну а раз без экспертного знания не обойтись, то почему бы саму модель и не выдрессировать работать с этим знанием?

Однозначно это приводит нас к расширенным вариационным автоэнкодерам ([Conditional Variational Autoencoder, CVAE](https://proceedings.neurips.cc/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf), когда вместе с реконструкцией автоэнкодера обучается еще и классификатор.

Однако, положа руку на сердце, CVAE это нечто очень близкое к обычным классификаторам, которые на раз-два уделают любой автоэнкодер, а какие конкурентные преимущества дает CVAE?
Если посмотреть на примеры реализаций [CVAE](https://github.com/nnormandin/Conditional_VAE/blob/master/Conditional_VAE.ipynb), то, кажется, никаких, только углеродный след усилим - вопросы интерпретации фич не решены, а точность классификатора и реконструктора хуже.

Размышляя над тем, как можно ситуацию улучшить, и перепробовав кучу подходов в духе dbscan/[нечеткой кластеризацией](https://ieeexplore.ieee.org/document/9177631)/градиентного бустинга над латентным слоем, как то сама собой родилась идея, что можно обучать VAE и выполнять кластеризацию над латентным вектором с помощью размеченных данных параллельно.
Для этого нужно в CVAE использовать в качестве модуля классификации штуку, которую обзовем _нечетким слоем_.

## Нечеткий слой

Сам термин "нечеткий" уже явно намекает на то, что в деле замешана нечеткая логика, по факту от которой были отщипнуты термы, а еще конкретнее - характеристические функции (или функции принадлежности).
Эти функции описывают степень принадлежности некоего входного значения $x$ к определенному нечеткому множеству, выражая эту степень от 0 (не принадлежит) до 1 (принадлежит полностью).
Разновидностей функций принадлежности существует огромное количество на любой вкус и цвет, но для простоты можно начать с классической [гауссианы](https://ru.wikipedia.org/wiki/%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%BE%D0%B2%D0%B0_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F), для одномерного случая имеющая вид

```math
\mu(x)=e^{-(\frac{x-a}{b\sqrt2})^2}.
```

Параметры $a$ и $b$ в этой формуле отвечают за смещение центра гауссианы относительно нуля и ее ширину, варьируя их можно более аккуратно подогнать гауссиану под конкретное нечеткое множество.
Приведенный одномерный случай прекрасен для понимания, чем ближе $x$ к $a$, тем выше его степень принадлежности к указанному множеству, а значение функции при этом ближе к 1.
Сам параметр $a$ описывает наиболее характного среднестатистического представителя конкретного множества.

Многомерные гауссианы, которые представляют для нас больший интерес, можно записать в виде
$\mu(x,A)=e^{-||[A.\~x]_{1\cdots m}||^2}$ где $m$ это размерность пространства, а  $A$ это [матрица переноса](https://en.wikipedia.org/wiki/Transformation_matrix):

```math
A_{(m+1) \times (m+1)} =
  \left[ {\begin{array}{cccc}
    s_{1} & a_{12} & \cdots & a_{1m} & c_{1}\\
    a_{21} & s_{2} & \cdots & a_{2m} & c_{2}\\
    \vdots & \vdots & \ddots & \vdots & c_{3}\\
    a_{m1} & a_{m2} & \cdots & s_{m} & c_{m}\\
    0 & 0 & \cdots & 0 & 1\\
  \end{array} } \right]
```

где $c_{1\cdots m}$ - это центроид гауссианы, $s_{1\cdots m}$ ее масштабирующие факторы, а $a_{1\cdots m, 1\cdots m}$ параметры _размещения_ гауссианы вокруг центроида.

Не касаясь некоторых аспектов ограничений на матрицу $A$ (например, подматрица, составленная из $s_{1\cdots m}$ и $a_{1\cdots m, 1\cdots m}$ должна быть положительно определенной), которые мы оставим на случай, если нарвемся на математиков или любителей линала, можно смело ожидать, что параметры этой матрицы с помощью волшебства обратного распространения ошибки в любимом ML-фреймворке подгонятся под какой-нибудь бугорок.
И входные векторы $x$ тоже закучкуются под нашу $\mu(x,A)$, только данные да разметку подавай.

![Двумерная гауссиана](fig2.png)
_Тот самый бугорок в 2D_

С каждым гауссианом связан отдельный класс в разметке, за который он отвечает, а совокупность таких гауссиан представляет собой _нечетким слоем_, в коде обозначенный как `FuzzyLayer`.

## Вживляем нечеткость в VAE

Как вы могли уже догадаться, `FuzzyLayer` приставленный к латентному слою VAE может формировать кластеры, связанные метками нечеткой термой.
И, кажется, такая процедура не сильно вмешивается в работу самого VAE, ведь и в самом деле, какая разница, где ему размещать свои распределения в латентном пространстве.
Мы лишь дополнительно подсказываем VAE размещать вектора средних для распределений так, чтобы они были сгруппированы и связаны с метками классов.

![было - стало](fig_before_after.png)

Для демонстрации применения `FuzzyLayer` давайте попрепарируем многострадальный MNIST и попробуем на нем построить CVAE-классификатор два в одном: классификатор для цифр и не отходя от кассы классификатор, который сообщает нам, есть ли в начертании цифры замкнутый кружочек.
Другими словами, `0,6,8,9` налево, а `1,2,3,4,5,7` направо.
Ради наглядности размерность латентного вектора выберем равной двойке, чтобы не напрягать воображение, разглядывая проекции многомерных разноцветных облачков.
Предполагаем, что читатель в достаточной мере знаком с технологией построения VAE\CVAE, поэтому некоторые детали просто опустим.
Благо, на хабре есть достаточно статей по теме, например [тут](https://habr.com/ru/articles/331382/)

Полный код блокнота доступен на [гитхабе](https://github.com/kenoma/pytorch-fuzzy/blob/main/experiments_habr_part_a.ipynb), обратим внимание на ряд моментов.

### Момент первый - кодирование меток

При предобработке датасетов целевые метки для классификации преобразуются в два вектора - вектор целевого значения выхода нечеткого слоя и маски.
Вектор маски это вектор той же размерности, что и целевой вектор, но на каждой позиции этого вектора стоит 0. если позиция не участвуюет в вычислении невязки и 1 - если участвует.
Маска, состоящая из одних нулей означает, что данный образец при обучении проходит через VAE, но при этом не вносит никакого вклада в структуру нечеткого слоя.

Например, для цифры `0`, полноценно участвующей в обучении CVAE, такое представление разметки под нашу задачу будет иметь вид:

||Цифра 0|Цифра 1|Цифра 2|Цифра 3|Цифра 4|Цифра 5|Цифра 6|Цифра 7|Цифра 8|Цифра 9|Цифра с кругляшком|Цифра без кругляшка|
|-|-|-|-|-|-|-|-|-|-|-|-|-|
|Целевой вектор|1|0|0|0|0|0|0|0|0|0|1|0|
|Маска|1|1|1|1|1|1|1|1|1|1|1|1|

Маска позволяет реализовать ряд нестандарных вещей, которые имеют смысл в сеттинге, когда VAE-компонента нашей модели строит компактные латентные представления входных векторов, а нечеткий слоя над этим дополнительно выполняет кластеризацию в соответствии с предоставляемым ему экспертным знанием.

Во-первых, с помощью маски можно пополнять разметку только для отдельно взятых классов.
Для цифры `0` это будет выглядеть так:

||Цифра 0|Цифра 1|Цифра 2|Цифра 3|Цифра 4|Цифра 5|Цифра 6|Цифра 7|Цифра 8|Цифра 9|Цифра с кругляшком|Цифра без кругляшка|
|-|-|-|-|-|-|-|-|-|-|-|-|-|
|Целевой вектор|1|0|0|0|0|0|0|0|0|0|0|0|
|Маска|1|0|0|0|0|0|0|0|0|0|0|0|

Обратите внимание, что признак `Цифра с кругляшком` мы тоже отключили.
Этот прием полезен, когда по каким-то причинам есть необходимость сосредоточиться на отдельно взятом классе или его аспекте.

Во-вторых, бывали ли у вас ситуации, когда вы смотрите на результаты классификации кошечек и вдруг видите что сетка сработала на что-то совершенно несуразное, и к кошакам отношение имеющее только общим нарративом? 
Можете не отвечать!
В примере с цифрой `0` такая ситуация бы выглядела так:

||Цифра 0|Цифра 1|Цифра 2|Цифра 3|Цифра 4|Цифра 5|Цифра 6|Цифра 7|Цифра 8|Цифра 9|Цифра с кругляшком|Цифра без кругляшка|
|-|-|-|-|-|-|-|-|-|-|-|-|-|
|Целевой вектор|0|0|0|0|0|0|0|0|0|0|0|0|
|Маска|1|0|0|0|0|0|0|0|0|0|0|0|

С помощью маски мы обозначили, что разметка относится только к классу `Цифра 0`, а в целевом векторе указали, что представленный пример точно не является цифрой 0.

Такие манипуляции с масками позволяют тюнить отдельно взятые гауссианы нечеткого слоя.
При этом для VAE-компоненты нашей модели без особой разницы - пропускаем мы данные с разметкой или без.

### Момент второй - функция потерь



Первая, это просто сложить все лоссы в один и бекпропнуть его.

```python
    loss = loss_recon + loss_kl + loss_fuzzy
```

Хороший прием, который работает всегда, когда у нас достаточно большое количество размеченных данных и мы можем в каждом батче подсунуть приличное количество размеченных данных к неразмеченным.
Однако, в ситуациях, когда у нас с размеченными данными все грустно, имеет смысл гонять сначала лосс от VAE, а потом вторым проходом уже лосс нечеткого классификатора.
Иногда помогает.

### Момент второй - модель

Ниже приведем код, описывающий компоненты нашей модели и подсветим пару моментов.

```python
class Encoder(nn.Module):
    """
    Компонент энкодера для VAE
    
    Args:
        latent_dim (int): Размер латентного вектора.
    """
    
    def __init__(self, latent_dim):
        super(Encoder, self).__init__()
                
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=5),
            nn.SiLU(),  
            nn.Conv2d(8, 16, kernel_size=5),
            nn.SiLU(),  
            nn.Conv2d(16, 32, kernel_size=5),
            nn.SiLU(),  
            nn.Conv2d(32, 64, kernel_size=5),
            nn.BatchNorm2d(64),
            nn.SiLU(),  
            nn.Flatten(),
            nn.Linear(9216, 625),
            nn.BatchNorm1d(625),
            nn.SiLU(),  
            nn.Linear(625, 2 * latent_dim), # mean + variance.
        )
        self.softplus = nn.Softplus()
         
    def forward(self, x, eps: float = 1e-8):
        """
        Выход энкодера для чистого VAE.
        
        Args:
            x (torch.Tensor): Входной вектор.
            eps (float): Небольшая поправка к скейлу для лучшей сходимости и устойчивости.
        
        Returns:
            mu, logvar, z, dist
        """

        x = self.encoder(x)
        mu, logvar = torch.chunk(x, 2, dim=-1)
        scale = self.softplus(logvar) + eps
        scale_tril = torch.diag_embed(scale)
        dist = torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)
        z = dist.rsample()

        return mu, logvar, z

class Decoder(nn.Module):
    """
    Компонент декодера для VAE
    
    Args:
        latent_dim (int): Размер латентного вектора.
    """
    
    def __init__(self, latent_dim):
        super(Decoder, self).__init__()
                
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 625),
            nn.BatchNorm1d(625),
            nn.SiLU(), 
            nn.Linear(625, 9216),
            nn.BatchNorm1d(9216),
            nn.SiLU(), 
            nn.Unflatten(1,(64, 12, 12)),
            nn.ConvTranspose2d(64, 32, 5),
            nn.SiLU(), 
            nn.ConvTranspose2d(32, 16, 5),
            nn.SiLU(), 
            nn.ConvTranspose2d(16, 8, 5),
            nn.SiLU(), 
            nn.ConvTranspose2d(8, 1, 5),
            nn.Sigmoid() 
        )
         
    def forward(self, z):
        """
        Декодирует латентный вектор в исходное представление
        
        Args:
            z (torch.Tensor): Латентный вектор.
        
        Returns:
            x
        """

        x = self.decoder(z)
        
        return x

class CVAE(nn.Module):
    """
    Conditional Variational Autoencoder (C-VAE) 
    
    Args:
        latent_dim (int): Размер латентного вектора.
        labels_count (int): Количество выходов классификатора
    """
    def __init__(self, latent_dim, labels_count):
        super(CVAE, self).__init__()

        self.encoder = Encoder(latent_dim)        
        self.decoder = Decoder(latent_dim)

        self.fuzzy = nn.Sequential(
             FuzzyLayer.fromdimentions(latent_dim, labels_count, trainable=True)

        )
        
    def forward(self, x):
        """
        Возвращает компоненты внутренних слоев CVAE, результаты реконструкции и классификации
        
        Args:
            x (torch.Tensor): Входной вектор.
        
        Returns:
            mu, x_recon, labels
        """

        mu, _,  _, = self.encoder(x)
        x_recon = self.decoder(mu)
        labels = self.fuzzy(mu)

        return mu, x_recon, labels
    
    def half_pass(self, x):
        """
        Возвращает результаты работы энкодера и классификатора
        """
        mu, logvar, z = self.encoder(x)
        labels = self.fuzzy(mu)

        return mu, logvar, z, labels
    
    def decoder_pass(self, x):
        return self.decoder(x)

```

Внимательный и опытный взгляд моментально обнаружит, что в энкодере и декодере нет вообще никаких отличий от VAE, вся магия происходит в классе `CVAE`.
Тот же внимательный взгляд обнаружит, что и в `CVAE` нет никаких принципиальных отличий от структуры Conditional VAE сетей, только в качестве классификатора используется `FuzzyLayer`.

Но все же пару моментов, хотелось бы отметить:

- Использование `SiLU` отдает дань модным-молодежным течениям, с этой активацией сеть действительно способна обучиться лучше

- Компонент `eps` в экнодере является трюком, повышающий стабильность и сходимость VAE в целом

- Нормализация батчей это ингридиент без которого порой обучение VAE не будет плодотворным

В целом, структура сети непринципиальна, для демонстрационных целей сойдет и такая, но в общем случае, инвестиции в подбор лучшей топологии сети для задачи себя оправдывают практически всегда.
В самое процедуре обучения никаких особых хитростей нет, разве что мы используем градиентный клиппинг для того, чтобы немного замедлить скорость сходимости весов сетки в надежде на лучшие итоговые результаты.

### Картинки и результаты экспериментов на полностью размеченном датасете

Для начала обучим чистую VAE для того, чтобы понять ориентиры.
После 50 итераций обучения лоссы для VAE на тестовой выборке имеют вид.

![Лоссы чистого VAE](fig_pure_vae_loss.png)

А латентный вектор, раскрашенный метками настоящих классов имеет следующий вид.

![Латентный вектор VAE](fig_pure_vae_latent.png)

Видно кластерную структуру, которая в какой то степени соотносится с реальными метками, при этом кластеры налезают друг на друга.
В общем, ничего интересного.
Теперь давайте обучим нашу модель на полном размеченном датасете 10 классов цифр, а 2 класса очертаний пока отключим через маску.

![Лоссы полного CVAE](fig_cvae_loss_10.png)

По лоссам VAE принципиальных разницы нет, общий лосс разве что выше из-за того, что к нему прибавился компонент `loss_fuzzy`.
Точность классификации 10 цифр на тестовой выборке составила по итогу ~96.52%.
Прежде чем вы с хохотом закроете эту статью, обращаю ваше внимание, что это выполнено всего на размерности латентного вектора 2.
Очевидно, что повышая размерность этого вектора можно добиться и лучших результатов.

![Латентные вектора CVAE](fig_cvae_latent_10.png)

И наконец, давайте включим все 12 классов, к 10 цифрам добавим еще два очертательных класса и посмотрим что будет.

![Лоссы полного CVAE 2](fig_cvae_loss_12.png)

Хм, VAE лоссы плюс минус те же, но точность классификации 10 цифр на тестовой выборке составила уже ~97.52%, на целый процент выше.
Сейчас разбираться с этим явлением не будем, не об этом статья, но забегая вперед скажу, что CVAE с нечетким классификатором действительно работает тем лучше, чем больше разнообразной экспертной информации о классе мы предоставляем.

![Латентные вектора CVAE](fig_cvae_latent_12.png)

Структура латентных векторов ожидаемо реорганизована очевидным образом, причем легко заметить тот факт, что все наши классы с округлыми замыкающими очертаниями примыкают к кластеру четвертки, у которой есть варианты замкнутого начертания с закрытым вверхом.

Давайте попробуем осмысленно попутешествовать теперь по нашим латентным пространствам и попробуем перейти от 9 к 4 и выберем два маршрута, один поближе к месту перемеживания классов очертаний, другой подальше.

![Генерация](fig_9_to_4_paths.png)

Видно, что по пути, который ближе к точке соприкосновения классов очертаний мы получили генерацию объектов с замкнутыми контурами, в то время как вдали - контуры разомнкуты.

![Генерация](fig_path_02.png)
![Генерация](fig_path_1.png)

Идея заключается в том, что если сделать хорошую разметку различными признаками, то обучив CVAE с нечеткими слоями мы получаем в распоряжение инструмент для генерации объектов в заданными признаками.
Добавим, что фактические домены для изменения параметров, привязанных к тому или иному классу-признаку можно получить из соответствующих терм, решив несложную систему линейных уравнений. 
Но мы оставим это в качестве домашнего задания тем, кто дочитал до этого места.

### Частично размеченные датасеты

Для демонстрации работы с частично размеченными датасетами просто приведем зависимости точности 10 циферного классификатора на тестовом датасете от процента неразмеченных данных.

![Точность от неразмеченных](fig_acc_unknown.png)

На этом графике с одной стороны видно, что более менее приемлемых результатов можно добиваться и на половине размеченного датасета.
А с другой - чтобы добиться сколько нибудь значимого улучшения модели, требуется еще столько же размеченных данных.
В целом, приведенный прием можно использовать как инструмент для оценки достаточности разметки датасета.

Ключевой момент в этой истории с неразмеченными данными заключается в том, что даже если образец датасета неразмечен, он по прежнему представляет ценность для формирования более точного VAE.

## Заключение

Вот так вот с помощью необоснованных предположений, эвристик и манипуляций со здравым смыслом нам удалось получить суперкомбайн, который может одновременно сжимать данные, классифицировать их и генерировать новые целенаправленно осмысленным образом.
В сухом остатке мы получили модель нечеткую модель CVAE, которая позволяет реорганизовать структуру латентного пространства максимально щадящим для VAE образом, сохраняя ее кластерную структуру.
Это в свою очередь дает возможность с помощью экспертного знания понять где есть что в этом латентном пространстве, что полезно при управляемой генерации новых объектов.
С другой стороны, сохранение кластерной структуры и группировка кластеров по внешним признакам позволяет рассматривать этот инструмент еще и как способ борьбы с частично размеченными датасетами.
Отметим также, что с помощью данного инструмента можно решать и задачи поиска аномалий, причем, помимо вполне очевидного способа выставить на все выходы `FuzzyLayer` порог и все что не пробило ни один - относить к подозрительным, есть еще один подход, который я оставлю на будущую публикацию, если вдруг эта зайдет.

За помощь в данной публикции спасибо @dnlhov, вместе с которым мы эту тему ковыряем до продуктового состояния.