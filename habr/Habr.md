# Варим кашу из нечеткой логики и вариационных автоэнкодеры 

Когда то давным давно, когда вариационные автоэнкодеры (VAE) только только появились, для меня это было как гром среди синего неба - наконец-то сетки смогут извлекать знание из данных, представляя их в виде параметризированных моделей, как это делают в формулах физики\математики!
Воображение рисовало возможностей изменением крутилок отращивать волосы нужной длины на фотографиях людей или извекать из тензометрических данных параметры, зависящие от температуры среды, но...

![Картинка из [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)](fig1.png)
_Картинка из [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)_

Реальность же оказалась сложнее, богаче и разнообразнее на сложности и разочарования, одним из которых явилось то, что VAE, даже в продвинутой вариации $\beta$-VAE, в скрытом слое выдает набор параметров, интерпретация которых требует перебора этих параметров в некоторой заранее неизвестной допустимой области изменения этих параметров и экспертного мнения для отслеживания. 
Покрутил параметр и увидел что у тебя стульчик на картинке завращался в ответ - отлично, параметр вращения.
Изменил другой параметр - размер стульчика начал меняться и так далее.
Хорошо, если задача довольно простая и не требует особых экспертных познаний в предметной области, однако такое бывает далеко не всегда, скорее даже чаще приходится решать задачи в которых экспертные знания скудны или на вес золота.
Ну раз без экспертного мнения не обойтись при обучении VAE моделек, то почему бы его и не предоставлять?
Так родилась идея CVAE, когда вместе с реконструкцией автоэнкодера обучается еще и классификатор.
Однако, CVAE это нечто очень близкое к обычным классификаторам, которые на раз-два уделают любой автоэнкодер, потому что последнему кроме классификации нужно еще и реконструкцию обеспечить на должном уровне.
Да и вопросы интерпретации фич особо никуда не делись.
С чем здорово помогает CVAE так это с частично размеченными датасетами, когда мы можем существенно ускорить процедуру разметки оставшегося датасета подсказками от CVAE.

Пока весь мир затаив дыхание следит за большими языковыми моделями и одни грезят о том, как подсадят всех на свои сервисы LLM, а другие прикидывают как можно сократить с их помощью если не зажравшихся айтишников, то хотя бы бухгалтеров, обычным машинистам по щиколотку в коричневой жиже дипленинга приходится решать вполне себе промышленно-бытовые задачи, которые требуются вот уже сейчас.
Наша команда - не исключение и решая очередную плохо обсусловленную практическую задачу из разряда "найдите в датасете картинок с исправными пружинами пружины с дефектами" сформировалась идея применения элементов нечеткой логики для построения CVAE.

Развитие этой идеи привело к реализации специального слоя для нейросетевых моделей, который в связке с VAE позволяет построить модели, пригодные к обучению на частично размеченных датасетах и раскрывающие информацию о кластерной структуре латентного вектора. Последнее может быть полезным как само по себе для задач классификации\анализа параметров, так и для целенаправленной генерации реконструированных объектов.

Весь код проекта и примеров к статье находится в репозиториях для [Keras](https://github.com/kenoma/KerasFuzzy) и для [pytorch](https://github.com/kenoma/pytorch-fuzzy).

## Нечеткий слой

Прежде чем переходить к нейросетевым моделям, попробую на пальцах объяснить, что такое нечеткий слой.
Сам термин "нечеткий" уже явно намекает на то, что в деле замешана нечеткая логика, по факту от которой были отщипнуты термы, а еще конкретнее - характеристические функции (или функции принадлежности).
Эти функции описывают степень принадлежности некоего входного занчения $x$ к определенному нечеткому множеству, выражая эту степень от 0 (не принадлежит) до 1 (принадлежит полностью).
Разновидностей функций принадлежности существует огромное количество на любой вкус и цвет, но для простоты можно начать с классической [гауссианы](https://ru.wikipedia.org/wiki/%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%BE%D0%B2%D0%B0_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F), для одномерного случая имеющая вид $\mu(x)=e^{-(\frac{x-a}{b\sqrt2})^2}$.

Параметры $a$ и $b$ в этой формуле отвечают за смещение центра гауссианы относительно нуля и ее ширину, варьируя их можно более аккуратно подогнать гауссиану под конкретное нечеткое множество.
Приведенный одномерный случай прекрасен для понимания - интуитивно понятно, что чем ближе $x$ к $a$, тем выше его степень принадлежности к указанному множеству, а сам параметр $a$ описывает наиболее характного среднестатистического представителя множества.
Практически же ценными являются многомерные гауссианы, которые мы представляем в виде
$\mu(x,A)=e^{-||[A.\~x]_{1\cdots m}||^2}$ где $m$ это размерность целевого пространства, а  $A$ это [матрица переноса](https://en.wikipedia.org/wiki/Transformation_matrix):

```math
A_{(m+1) \times (m+1)} =
  \left[ {\begin{array}{cccc}
    s_{1} & a_{12} & \cdots & a_{1m} & c_{1}\\
    a_{21} & s_{2} & \cdots & a_{2m} & c_{2}\\
    \vdots & \vdots & \ddots & \vdots & c_{3}\\
    a_{m1} & a_{m2} & \cdots & s_{m} & c_{m}\\
    0 & 0 & \cdots & 0 & 1\\
  \end{array} } \right]
```

где $c_{1\cdots m}$ - это центроид гауссианы, $s_{1\cdots m}$ ее масштабирующий фактор, а $a_{1\cdots m, 1\cdots m}$ параметры вращения гауссианы вокруг центроида.
Не касаясь некоторых аспектов ограничений на матрицу $A$ (например, подматрица из $s_{1\cdots m}$ и $a_{1\cdots m, 1\cdots m}$ должна быть положительно определенной), которые мы оставим для научных работ, можно смело заявлять, что в таком виде мы можем это все дело засунуть в популярный ML-фреймворк и с помощью волшебства обратного распространения градиента затюнить вусмерть каким пожелаем образом.
![Двумерная гауссиана](fig2.png)
_Пример двумерной гауссианы_

Другими словами, наша гауссиана это некая функция, которая входным векторам $x$ выдает значения тем ближе к 1, чем ближе они находятся к центроиду гауссианы и тем ближе к 0, чем дальше они от $c_{1\cdots m}$.

Совокупность таких многомерных гауссиан мы объединили в нечеткий слой, который в коде обозначили как `FuzzyLayer`.

Самая главная фишка `FuzzyLayer` заключается в том, что каждый элемент слоя стремиться сгруппировать под собой все входные значения, отвечающими за сработку того или выходного признака, действуя тем самым как кластеризатор.

## Вживляем нечеткость в VAE 

Как вы могли уже догадаться, наш `FuzzyLayer` приставленный к латентному слою VAE может группировать его значения в определенных областях, формируя кластеры связанных с нечеткой термой.
И что самое интересное, такая группировка идейно не сильно вмешивается в работу самого VAE, ведь и в самом деле, какая разница, где ему размещать свои кучки распределений.

[было - стало]

Чтобы продемонстрировать работу `FuzzyLayer` давайте попрепарируем многострадальный MNIST и попробуем на нем получить классификатор для цифр и не отходя от кассы классификатор, который сообщает нам, есть в начертании цифры замкнутый контур.
Другими словами, `0,6,8,9` налево, `1,2,3,4,5,7` направо.
Латентный вектор будет размерности 2 для того, чтобы получить красивые наглядные картиночки, а за точностью и комариными хоботками размерность придется повышать.

Полный код блокнота с представленными экспериментами вы найдете на [гитхабе](https://github.com/kenoma/pytorch-fuzzy/blob/main/experiments_habr_part_a.ipynb), тут же дадим пояснения по некоторым моментам.

### Момент первый - функция потерь и кодирование меток

При загрузке данных целевые метки для классификации мы представляем в виде двух векторов - целевого значения и маски. Целевое значение это вектор состояния, который мы хотим получить на выходе из классификатора.

```python

def get_target_and_mask(target_label, unknown_ratio):
    """
    Возвращает вектор целевого значения и маску в виде сдвоенного тензора

    Args:
        target_label (int): Метка класса
        unknown_ratio (float): Доля примеров в датасете, чья разметка будет игнорироваться при обучении
    
    Returns:
        tensor (2, 12)
    """
    t = F.one_hot(torch.LongTensor([target_label]), 12)
    if (target_label == 0) or (target_label == 6) or (target_label == 8) or (target_label == 9):
        t[0][10] = 1
    else:
        t[0][11] = 1
    m = torch.ones((1, 12)) if torch.rand(1) > unknown_ratio else torch.zeros((1, 12))
    return torch.cat((t, m), 0)

# загружаем обучающую выборку
train_data = datasets.MNIST(
    '~/.pytorch/MNIST_data/', 
    download=True, 
    train=True, 
    transform = transform,
    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x, unknown_classes_ratio))
)

# загружаем тестовую выборку
test_data = datasets.MNIST(
    '~/.pytorch/MNIST_data/', 
    download=True, 
    train=False, 
    transform=transform, 
    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x, 0))
)
```

Например, для цифры `0` такой вектор имеет вид `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]`, первые 10 значений в этом векторе отведены унитарному коду цифры, а последние два - признаку наличия замкнутых контуров.
Вектор маски это вектор той же размерности, что и целевой вектор, но на каждой позиции этого вектора стоит 0. если позиция не участвуюет в вычислении невязки и 1 - если участвует.
Другими словами, если маска состоит из единиц, то весь целевой вектор участвует в вычислении лосса, а если из нулей - то значения целевого вектора игнорируются, какими бы они ни были.
В последнем случае VAE выполняет проход обучения по обычной схеме, без какого бы то ни было влияния со стороны классификатора.

```python
def compute_loss(..., target_labels, predicted_labels):
    ...
    target_firings = target_labels[:,0,:]
    mask = target_labels[:,1,:]
    loss_fuzzy = (mask * torch.square(target_firings - predicted_labels)).sum(-1).mean()
    ...
```

Фокусы с маской позволяют учинить ряд нестандарных вещей, помимо полного включения и выключения элемента выборки в проходе классификатора.

Во-первых, с помощью маски можно пополнять разметку только для отдельно взятых классов, например, вы посмотрели, на картинку и увидели, что там циферка, скажем, три, но написана так, что кажется, что смыкаются у нее линиии и нет уверенности, что на ней нет замкнутых контуров.
Тогда в маске можно поставить последние два значения нулевыми и термы, отвечающие за контуры цифр не сдвинутся с места при прогонке этого размеченного примера.
Плохо ль что-ли? Хорошо!
Во-вторых, бывали ли у вас ситуации, когда вы смотрите на результаты классификации кошечек и вдруг видите что сетка сработала на что-то совершенно несуразное, и к кошакам отношение имеющее только общим нарративом? Можете не отвечать!
Для борьбы с подобного рода явлениями можно поставить `0` в соответствующей позиции целевого вектора, а в маске `1` только для этой же позиции.
Этот маневр позволит отодвинуть нашу терму с кошками подальше от этой несуразности, не влияя на остальные термы. 
При этом для VAE такие образцы будут полезны - пусть учится фичи молотить, да аномалии находить.
Про детектор аномалий в этот раз не будем, а пока что завершим с лоссом.
Тут, как водится, есть две стратегии.
Первая, это просто сложить все лоссы в один и бекпропнуть его.

```python
    loss = loss_recon + loss_kl + loss_fuzzy
```

Хороший прием, который работает всегда, когда у нас достаточно большое количество размеченных данных и мы можем в каждом батче подсунуть приличное количество размеченных данных к неразмеченным.
Однако, в ситуациях, когда у нас с размеченными данными все грустно, имеет смысл гонять сначала лосс от VAE, а потом вторым проходом уже лосс нечеткого классификатора. 
Иногда помогает.
Вообще, в этой статье для простоты восприятия у нас будет только два сценария с масками - размеченный образец либо полностью включен, либо полностью выключен.

### Момент второй - модель

Ниже приведем код, описывающий компоненты нашей модели и подсветим пару моментов.

```python
class Encoder(nn.Module):
    """
    Компонент энкодера для VAE
    
    Args:
        input_dim (int): Размерность входных данных.
        hidden_dim (int): Размер внутренних слоев.
        latent_dim (int): Размер латентного вектора.
    """
    
    def __init__(self, latent_dim):
        super(Encoder, self).__init__()
                
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=5),
            nn.SiLU(),  
            nn.Conv2d(8, 16, kernel_size=5),
            nn.SiLU(),  
            nn.Conv2d(16, 32, kernel_size=5),
            nn.SiLU(),  
            nn.Conv2d(32, 64, kernel_size=5),
            nn.BatchNorm2d(64),
            nn.SiLU(),  
            nn.Flatten(),
            nn.Linear(9216, 625),
            nn.BatchNorm1d(625),
            nn.SiLU(),  
            nn.Linear(625, 2 * latent_dim), # mean + variance.
        )
        self.softplus = nn.Softplus()
         
    def forward(self, x, eps: float = 1e-8):
        """
        Выход энкодера для чистого VAE.
        
        Args:
            x (torch.Tensor): Входной вектор.
            eps (float): Небольшая поправка к скейлу для лучшей сходимости и устойчивости.
        
        Returns:
            mu, logvar, z, dist
        """

        x = self.encoder(x)
        mu, logvar = torch.chunk(x, 2, dim=-1)
        scale = self.softplus(logvar) + eps
        scale_tril = torch.diag_embed(scale)
        dist = torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)
        z = dist.rsample()

        return mu, logvar, z

class Decoder(nn.Module):
    """
    Компонент декодера для VAE
    
    Args:
        input_dim (int): Размерность входных данных.
        hidden_dim (int): Размер внутренних слоев.
        latent_dim (int): Размер латентного вектора.
    """
    
    def __init__(self, latent_dim):
        super(Decoder, self).__init__()
                
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 625),
            nn.BatchNorm1d(625),
            nn.SiLU(), 
            nn.Linear(625, 9216),
            nn.BatchNorm1d(9216),
            nn.SiLU(), 
            nn.Unflatten(1,(64, 12, 12)),
            nn.ConvTranspose2d(64, 32, 5),
            nn.SiLU(), 
            nn.ConvTranspose2d(32, 16, 5),
            nn.SiLU(), 
            nn.ConvTranspose2d(16, 8, 5),
            nn.SiLU(), 
            nn.ConvTranspose2d(8, 1, 5),
            nn.Sigmoid() 
        )
         
    def forward(self, z):
        """
        Декодирует латентный вектор в исходное представление
        
        Args:
            z (torch.Tensor): Латентный вектор.
        
        Returns:
            x
        """

        x = self.decoder(z)
        
        return x

class CVAE(nn.Module):
    """
    Conditional Variational Autoencoder (C-VAE) 
    
    Args:
        input_dim (int): Размерность входных данных.
        hidden_dim (int): Размер внутренних слоев.
        latent_dim (int): Размер латентного вектора.
    """
    def __init__(self, latent_dim, labels_count):
        super(CVAE, self).__init__()

        self.encoder = Encoder(latent_dim)        
        self.decoder = Decoder(latent_dim)

        self.fuzzy = nn.Sequential(
             FuzzyLayer.fromdimentions(latent_dim, labels_count, trainable=True)

        )
        
    def forward(self, x):
        """
        Возвращает компоненты внутренних слоев CVAE, результаты реконструкции и классификации
        
        Args:
            x (torch.Tensor): Входной вектор.
        
        Returns:
            mu, x_recon, labels
        """

        mu, _,  _, = self.encoder(x)
        x_recon = self.decoder(mu)
        labels = self.fuzzy(mu)

        return mu, x_recon, labels
    
    def half_pass(self, x):
        """
        Возвращает результаты работы энкодера и классификатора
        """
        mu, logvar, z = self.encoder(x)
        labels = self.fuzzy(mu)

        return mu, logvar, z, labels
    
    def decoder_pass(self, x):
        return self.decoder(x)

```

Внимательный и опытный взгляд моментально обнаружит, что в энкодере и декодере нет вообще никаких отличий от VAE, вся магия происходит в классе `CVAE`.
Тот же внимательный взгляд обнаружит, что и в `CVAE` нет никаких принципиальных отличий от структуры Conditional VAE сетей, только в качестве классификатора используется `FuzzyLayer`.
Посмотреть, как работают обычные CVAE без нечетких прибамбасов можно [тут](https://github.com/nnormandin/Conditional_VAE/blob/master/Conditional_VAE.ipynb)
Но все же пару моментов, хотелось бы отметить:

- Использование `SiLU` отдает дань модным-молодежным течениям, с этой активацией сеть действительно способна обучиться лучше

- Компонент `eps` в экнодере является трюком, повышающий стабильность и сходимость VAE в целом

- Нормализация батчей это ингридиент без которого порой обучение VAE не будет плодотворным

В целом, структура сети непринципиальна, для демонстрационных целей сойдет и такая, но в общем случае, инвестиции в подбор лучшей топологии сети для задачи себя оправдывают практически всегда.

